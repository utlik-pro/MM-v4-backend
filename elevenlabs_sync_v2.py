#!/usr/bin/env python3
"""
ElevenLabs Sync v2 - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –∞–≥–µ–Ω—Ç–æ–º

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
1. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å: conversation_config.agent.prompt.knowledge_base
2. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–æ —Ö–µ—à—É)
3. –ó–∞–º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –Ω–∞ –Ω–æ–≤—ã–µ (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç)
4. –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ KB –ø–æ—Å–ª–µ –æ—Ç–≤—è–∑–∫–∏ –æ—Ç –∞–≥–µ–Ω—Ç–∞
"""

import os
import sys
import json
import time
import hashlib
import argparse
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env –µ—Å–ª–∏ –µ—Å—Ç—å
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_KEY = os.environ.get('ELEVENLABS_API_KEY')
AGENT_ID = os.environ.get('ELEVENLABS_AGENT_ID')
BASE_URL = "https://api.elevenlabs.io/v1"

# –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º)
PERMANENT_DOCS = {
    '–ú–ë–ê', '00-obschie-svedeniya', '01-obrazovatelnaya-infrastruktura',
    '02-parkinki-i-sport', '03-finansovye-uslugi', '03-empathy-enhancer',
    '05-sroki-sdachi-domov'
}

STATE_FILE = Path('./quarters_state.json')


def log(msg: str):
    """–í—ã–≤–æ–¥ —Å timestamp"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}", flush=True)


def get_headers() -> dict:
    return {"xi-api-key": API_KEY}


def load_state() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"quarters": {}, "permanent_docs": {}}


def save_state(state: dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    state["last_update"] = datetime.now().isoformat()
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def calculate_hash(file_path: str) -> str:
    """MD5 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()


def get_agent_kb() -> List[Dict]:
    """–ü–æ–ª—É—á–∏—Ç—å knowledge_base –∞–≥–µ–Ω—Ç–∞ (–ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!)"""
    url = f"{BASE_URL}/convai/agents/{AGENT_ID}"
    resp = requests.get(url, headers=get_headers(), timeout=60)
    
    if resp.status_code != 200:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞: {resp.status_code}")
        return []
    
    data = resp.json()
    # –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!
    kb = data.get('conversation_config', {}).get('agent', {}).get('prompt', {}).get('knowledge_base', [])
    return kb


def upload_document(file_path: str, name: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ KB –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é"""
    url = f"{BASE_URL}/convai/knowledge-base"
    
    with open(file_path, 'rb') as f:
        files = {'file': (f"{name}.md", f, 'text/markdown')}
        resp = requests.post(url, headers=get_headers(), files=files, timeout=120)
    
    if resp.status_code in [200, 201]:
        doc_id = resp.json().get('id')
        log(f"      üì§ –ó–∞–≥—Ä—É–∂–µ–Ω: {doc_id[:20]}...")
        
        # –í–ê–ñ–ù–û: –ó–∞–ø—É—Å–∫–∞–µ–º RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —è–≤–Ω–æ!
        # ElevenLabs –ù–ï –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ API
        trigger_rag_indexing(doc_id)
        
        return doc_id
    else:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: {resp.status_code}")
        return None


def trigger_rag_indexing(doc_id: str) -> bool:
    """–Ø–≤–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    ElevenLabs –ù–ï –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ API!
    –ù—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å POST /convai/knowledge-base/{id}/rag-index
    """
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}/rag-index"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å —á—Ç–æ —É –∞–≥–µ–Ω—Ç–∞
    data = {
        "model": "multilingual_e5_large_instruct"
    }
    
    try:
        resp = requests.post(
            url, 
            headers={**get_headers(), "Content-Type": "application/json"},
            json=data,
            timeout=60
        )
        
        if resp.status_code in [200, 201, 202]:
            log(f"      ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞")
            return True
        else:
            log(f"      ‚ö†Ô∏è  –°—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {resp.status_code}")
            return False
            
    except Exception as e:
        log(f"      ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return False


def check_indexing_status(doc_id: str) -> str:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    ElevenLabs API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    {
        "indexes": [
            {"status": "succeeded", "progress_percentage": 100.0, ...}
        ]
    }
    """
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}/rag-index"
    
    try:
        resp = requests.get(url, headers=get_headers(), timeout=30)
        if resp.status_code == 200:
            data = resp.json()
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å: indexes[0].status
            indexes = data.get('indexes', [])
            if indexes:
                return indexes[0].get('status', 'unknown')
            return 'no_index'
        return 'error'
    except:
        return 'error'


def wait_for_indexing(doc_id: str, max_wait: int = 120) -> bool:
    """–î–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    Args:
        doc_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        max_wait: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    Returns:
        True –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, False –µ—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç
    """
    start = time.time()
    
    while time.time() - start < max_wait:
        status = check_indexing_status(doc_id)
        
        # succeeded - —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω (ElevenLabs API)
        if status in ['succeeded', 'indexed', 'completed', 'ready']:
            return True
        elif status in ['error', 'failed']:
            log(f"      ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
            return False
        
        # –ñ–¥—ë–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞
        time.sleep(5)
    
    log(f"      ‚ö†Ô∏è  –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    return False


def update_agent_kb(new_kb: List[Dict]) -> bool:
    """–û–±–Ω–æ–≤–∏—Ç—å KB –∞–≥–µ–Ω—Ç–∞ (–ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!)"""
    url = f"{BASE_URL}/convai/agents/{AGENT_ID}"
    
    update_data = {
        "conversation_config": {
            "agent": {
                "prompt": {
                    "knowledge_base": new_kb
                }
            }
        }
    }
    
    log(f"   üì§ PATCH –∑–∞–ø—Ä–æ—Å ({len(new_kb)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
    
    # Retry —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
    for attempt in range(3):
        try:
            resp = requests.patch(
                url,
                headers={**get_headers(), "Content-Type": "application/json"},
                json=update_data,
                timeout=(30, 600)  # 30s connect, 10min read (–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–æ–π)
            )
            
            if resp.status_code == 200:
                log(f"   ‚úÖ –ê–≥–µ–Ω—Ç –æ–±–Ω–æ–≤–ª—ë–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                log(f"   ‚ùå –û—à–∏–±–∫–∞: {resp.status_code} - {resp.text[:200]}")
                return False
                
        except requests.exceptions.Timeout:
            log(f"   ‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3: —Ç–∞–π–º–∞—É—Ç")
            if attempt < 2:
                time.sleep(5)
        except Exception as e:
            log(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return False
    
    log("   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
    return False


def delete_document(doc_id: str) -> bool:
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ KB"""
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}"
    resp = requests.delete(url, headers=get_headers(), timeout=30)
    return resp.status_code in [200, 204]


def init_state_from_agent(agent_docs: dict, quarters_path: Path) -> dict:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å state –∏–∑ —Ç–µ–∫—É—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞"""
    state = {"quarters": {}, "permanent_docs": {}}
    
    for name, doc in agent_docs.items():
        md_file = quarters_path / f"{name}.md"
        content_hash = ""
        if md_file.exists():
            content_hash = calculate_hash(str(md_file))
        
        doc_info = {
            "doc_id": doc.get('id', ''),
            "content_hash": content_hash,
            "last_updated": None
        }
        
        if name in PERMANENT_DOCS:
            state["permanent_docs"][name] = doc_info
        else:
            state["quarters"][name] = doc_info
    
    return state


def sync_quarters(quarters_dir: str = 'quarters', changed_files: List[str] = None, dry_run: bool = False):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    
    Args:
        quarters_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏
        changed_files: –°–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        dry_run: –¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
    """
    log("=" * 60)
    log("üöÄ ElevenLabs Sync v2")
    log("=" * 60)
    
    if not API_KEY or not AGENT_ID:
        log("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ELEVENLABS_API_KEY –∏ ELEVENLABS_AGENT_ID")
        return
    
    quarters_path = Path(quarters_dir)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º state –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
    state = load_state()
    
    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∞–≥–µ–Ω—Ç–∞
    log("\nüì• –®–∞–≥ 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞...")
    agent_kb = get_agent_kb()
    log(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∞–≥–µ–Ω—Ç–µ: {len(agent_kb)}")
    
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å name ‚Üí doc –¥–ª—è –∞–≥–µ–Ω—Ç–∞
    agent_docs = {doc['name']: doc for doc in agent_kb}
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è state –µ—Å–ª–∏ –ø—É—Å—Ç–æ–π
    if not state.get('quarters'):
        log("‚ö†Ô∏è  State –ø—É—Å—Ç–æ–π, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑ –∞–≥–µ–Ω—Ç–∞...")
        state = init_state_from_agent(agent_docs, quarters_path)
        save_state(state)
        log(f"   ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(state['quarters'])} –∫–≤–∞—Ä—Ç–∞–ª–æ–≤")
    
    # –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
    log("\nüîç –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
    
    files_to_update = []
    files_to_update_names = set()  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if changed_files:
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ changed_files
        unique_files = list(dict.fromkeys(changed_files))
        md_files = [quarters_path / f for f in unique_files if f.endswith('.md')]
    else:
        md_files = list(quarters_path.glob('*.md'))
    
    # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º force_update - –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ö–µ—à–∏
    # –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è Render –≥–¥–µ quarters_state.json –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º,
    # –Ω–æ –º—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –µ–≥–æ –∏–∑ –∞–≥–µ–Ω—Ç–∞ –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
    
    for md_file in md_files:
        name = md_file.stem  # –ò–º—è –±–µ–∑ .md
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if name in PERMANENT_DOCS:
            continue
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –ø—Ä–∏–≤—è–∑–∞–Ω—ã –∫ –∞–≥–µ–Ω—Ç—É
        if name not in agent_docs:
            log(f"   ‚è≠Ô∏è  {name} (–Ω–µ –≤ –∞–≥–µ–Ω—Ç–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
            continue
        
        current_hash = calculate_hash(str(md_file))
        saved_hash = state.get('quarters', {}).get(name, {}).get('content_hash', '')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ö–µ—à —Ñ–∞–π–ª–∞ –∏–∑–º–µ–Ω–∏–ª—Å—è –ò —Ñ–∞–π–ª –µ—â—ë –Ω–µ –≤ —Å–ø–∏—Å–∫–µ
        if current_hash != saved_hash and name not in files_to_update_names:
            files_to_update_names.add(name)
            files_to_update.append({
                'name': name,
                'path': str(md_file),
                'hash': current_hash,
                'old_doc_id': agent_docs.get(name, {}).get('id')
            })
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 8 —Å–∏–º–≤–æ–ª–æ–≤ —Ö–µ—à–µ–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            log(f"   üîÑ {name} (—Ö–µ—à: {saved_hash[:8] if saved_hash else 'empty'}‚Üí{current_hash[:8]})")
        else:
            log(f"   ‚úÖ {name} (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
    
    if not files_to_update:
        log("\n‚úÖ –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
        return
    
    log(f"\nüìä –ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é: {len(files_to_update)} —Ñ–∞–π–ª–æ–≤")
    
    if dry_run:
        log("\n‚ö†Ô∏è  DRY RUN - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        return
    
    # –®–∞–≥ 3: –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏
    log("\nüì§ –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π...")
    
    uploaded = []
    for file_info in files_to_update:
        log(f"   üì§ {file_info['name']}...", )
        
        new_doc_id = upload_document(file_info['path'], file_info['name'])
        if new_doc_id:
            file_info['new_doc_id'] = new_doc_id
            uploaded.append(file_info)
            log(f"   ‚úÖ {file_info['name']} ‚Üí {new_doc_id[:20]}...")
        else:
            log(f"   ‚ùå {file_info['name']} - –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
    
    if not uploaded:
        log("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
        return
    
    # –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    log("\n‚è≥ –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
    
    indexed = []
    for file_info in uploaded:
        doc_id = file_info['new_doc_id']
        name = file_info['name']
        
        log(f"   üîç {name}...", )
        
        # –ñ–¥—ë–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–º–∞–∫—Å 30 —Å–µ–∫—É–Ω–¥ ‚Äî –æ–±—ã—á–Ω–æ –æ—á–µ–Ω—å –±—ã—Å—Ç—Ä–æ)
        if wait_for_indexing(doc_id, max_wait=30):
            indexed.append(file_info)
            log(f"   ‚úÖ {name} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")
        else:
            # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –¥–æ–∂–¥–∞–ª–∏—Å—å - –¥–æ–±–∞–≤–ª—è–µ–º, –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è –≤ —Ñ–æ–Ω–µ
            indexed.append(file_info)
            log(f"   ‚ö†Ô∏è  {name} - –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ (–ø—Ä–æ–¥–æ–ª–∂–∏–º)")
    
    log(f"   üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(indexed)}")
    
    # –®–∞–≥ 5: –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞ (–∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ ID –Ω–∞ –Ω–æ–≤—ã–µ)
    log("\nü§ñ –®–∞–≥ 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π KB - –∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –Ω–æ–≤—ã–µ
    new_agent_kb = []
    old_doc_ids = []  # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    seen_names = set()  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    for doc in agent_kb:
        name = doc['name']
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π)
        if name in seen_names:
            log(f"   ‚ö†Ô∏è  {name}: –¥—É–±–ª–∏–∫–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            old_doc_ids.append(doc['id'])  # –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç
            continue
        seen_names.add(name)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        updated = next((f for f in indexed if f['name'] == name), None)
        
        if updated:
            # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
            new_agent_kb.append({
                'type': doc.get('type', 'text'),
                'name': name,
                'id': updated['new_doc_id'],
                'usage_mode': doc.get('usage_mode', 'auto')
            })
            if updated.get('old_doc_id'):
                old_doc_ids.append(updated['old_doc_id'])
            log(f"   üîÑ {name}: {updated.get('old_doc_id', 'new')[:15]}... ‚Üí {updated['new_doc_id'][:15]}...")
        else:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            new_agent_kb.append(doc)
    
    log(f"   üìä –ò—Ç–æ–≥–æ –≤ –∞–≥–µ–Ω—Ç–µ: {len(new_agent_kb)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞
    if not update_agent_kb(new_agent_kb):
        return
    
    # –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ KB
    if old_doc_ids:
        log("\nüóëÔ∏è  –®–∞–≥ 6: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π...")
        time.sleep(2)  # –î–∞—ë–º –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–≤—è–∑–∫—É
        
        for old_id in old_doc_ids:
            if delete_document(old_id):
                log(f"   ‚úÖ –£–¥–∞–ª—ë–Ω: {old_id[:20]}...")
            else:
                log(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª—ë–Ω: {old_id[:20]}...")
    
    # –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    log("\nüíæ –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è...")
    
    for file_info in indexed:
        state['quarters'][file_info['name']] = {
            'doc_id': file_info['new_doc_id'],
            'content_hash': file_info['hash'],
            'last_updated': datetime.now().isoformat()
        }
    
    save_state(state)
    
    # –ò—Ç–æ–≥–∏
    log("\n" + "=" * 60)
    log("üìä –ò–¢–û–ì–ò:")
    log(f"   üì§ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(uploaded)}")
    log(f"   ‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(indexed)}")
    log(f"   üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö: {len(old_doc_ids)}")
    log("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='ElevenLabs Sync v2')
    parser.add_argument('--dir', default='quarters', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è')
    parser.add_argument('--changed-files', type=str, help='–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')
    
    args = parser.parse_args()
    
    changed_files = None
    if args.changed_files:
        with open(args.changed_files, 'r') as f:
            changed_files = [line.strip() for line in f if line.strip()]
    
    sync_quarters(
        quarters_dir=args.dir,
        changed_files=changed_files,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()

