#!/usr/bin/env python3
"""
ElevenLabs Sync v2 - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –∞–≥–µ–Ω—Ç–æ–º

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
1. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å: conversation_config.agent.prompt.knowledge_base
2. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–æ —Ö–µ—à—É)
3. –ó–∞–º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –Ω–∞ –Ω–æ–≤—ã–µ (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç)
4. –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ KB –ø–æ—Å–ª–µ –æ—Ç–≤—è–∑–∫–∏ –æ—Ç –∞–≥–µ–Ω—Ç–∞
"""

import os
import sys
import json
import time
import hashlib
import argparse
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env –µ—Å–ª–∏ –µ—Å—Ç—å
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_KEY = os.environ.get('ELEVENLABS_API_KEY')
AGENT_ID = os.environ.get('ELEVENLABS_AGENT_ID')
BASE_URL = "https://api.elevenlabs.io/v1"

# –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º)
PERMANENT_DOCS = {
    '–ú–ë–ê', '00-obschie-svedeniya', '01-obrazovatelnaya-infrastruktura',
    '02-parkinki-i-sport', '03-finansovye-uslugi', '03-empathy-enhancer',
    '05-sroki-sdachi-domov'
}

STATE_FILE = Path('./quarters_state.json')


def log(msg: str):
    """–í—ã–≤–æ–¥ —Å timestamp"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}", flush=True)


def get_headers() -> dict:
    return {"xi-api-key": API_KEY}


def load_state() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"quarters": {}, "permanent_docs": {}}


def save_state(state: dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    state["last_update"] = datetime.now().isoformat()
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def calculate_hash(file_path: str) -> str:
    """MD5 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()


def get_agent_kb() -> List[Dict]:
    """–ü–æ–ª—É—á–∏—Ç—å knowledge_base –∞–≥–µ–Ω—Ç–∞ (–ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!)"""
    url = f"{BASE_URL}/convai/agents/{AGENT_ID}"
    resp = requests.get(url, headers=get_headers(), timeout=60)
    
    if resp.status_code != 200:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞: {resp.status_code}")
        return []
    
    data = resp.json()
    # –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!
    kb = data.get('conversation_config', {}).get('agent', {}).get('prompt', {}).get('knowledge_base', [])
    return kb


def upload_document(file_path: str, name: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ KB"""
    url = f"{BASE_URL}/convai/knowledge-base"
    
    with open(file_path, 'rb') as f:
        files = {'file': (f"{name}.md", f, 'text/markdown')}
        resp = requests.post(url, headers=get_headers(), files=files, timeout=120)
    
    if resp.status_code in [200, 201]:
        doc_id = resp.json().get('id')
        return doc_id
    else:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: {resp.status_code}")
        return None


def wait_for_indexing(doc_id: str, max_wait: int = 120) -> bool:
    """–ñ–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}"
    
    for _ in range(max_wait // 5):
        resp = requests.get(url, headers=get_headers(), timeout=30)
        if resp.status_code == 200:
            status = resp.json().get('metadata', {}).get('rag_index_status', '')
            if status == 'indexed':
                return True
        time.sleep(5)
    
    return False


def update_agent_kb(new_kb: List[Dict]) -> bool:
    """–û–±–Ω–æ–≤–∏—Ç—å KB –∞–≥–µ–Ω—Ç–∞ (–ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!)"""
    url = f"{BASE_URL}/convai/agents/{AGENT_ID}"
    
    update_data = {
        "conversation_config": {
            "agent": {
                "prompt": {
                    "knowledge_base": new_kb
                }
            }
        }
    }
    
    resp = requests.patch(
        url,
        headers={**get_headers(), "Content-Type": "application/json"},
        json=update_data,
        timeout=120
    )
    
    return resp.status_code == 200


def delete_document(doc_id: str) -> bool:
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ KB"""
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}"
    resp = requests.delete(url, headers=get_headers(), timeout=30)
    return resp.status_code in [200, 204]


def sync_quarters(quarters_dir: str = 'quarters', changed_files: List[str] = None, dry_run: bool = False):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    
    Args:
        quarters_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏
        changed_files: –°–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        dry_run: –¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
    """
    log("=" * 60)
    log("üöÄ ElevenLabs Sync v2")
    log("=" * 60)
    
    if not API_KEY or not AGENT_ID:
        log("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ELEVENLABS_API_KEY –∏ ELEVENLABS_AGENT_ID")
        return
    
    state = load_state()
    quarters_path = Path(quarters_dir)
    
    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∞–≥–µ–Ω—Ç–∞
    log("\nüì• –®–∞–≥ 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞...")
    agent_kb = get_agent_kb()
    log(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∞–≥–µ–Ω—Ç–µ: {len(agent_kb)}")
    
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å name ‚Üí doc –¥–ª—è –∞–≥–µ–Ω—Ç–∞
    agent_docs = {doc['name']: doc for doc in agent_kb}
    
    # –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
    log("\nüîç –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
    
    files_to_update = []
    
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if changed_files:
        md_files = [quarters_path / f for f in changed_files if f.endswith('.md')]
    else:
        md_files = list(quarters_path.glob('*.md'))
    
    for md_file in md_files:
        name = md_file.stem  # –ò–º—è –±–µ–∑ .md
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if name in PERMANENT_DOCS:
            continue
        
        current_hash = calculate_hash(str(md_file))
        saved_hash = state.get('quarters', {}).get(name, {}).get('content_hash', '')
        
        if current_hash != saved_hash:
            files_to_update.append({
                'name': name,
                'path': str(md_file),
                'hash': current_hash,
                'old_doc_id': agent_docs.get(name, {}).get('id')
            })
            log(f"   üîÑ {name} (—Ö–µ—à –∏–∑–º–µ–Ω–∏–ª—Å—è)")
        else:
            log(f"   ‚úÖ {name} (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
    
    if not files_to_update:
        log("\n‚úÖ –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
        return
    
    log(f"\nüìä –ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é: {len(files_to_update)} —Ñ–∞–π–ª–æ–≤")
    
    if dry_run:
        log("\n‚ö†Ô∏è  DRY RUN - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        return
    
    # –®–∞–≥ 3: –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏
    log("\nüì§ –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π...")
    
    uploaded = []
    for file_info in files_to_update:
        log(f"   üì§ {file_info['name']}...", )
        
        new_doc_id = upload_document(file_info['path'], file_info['name'])
        if new_doc_id:
            file_info['new_doc_id'] = new_doc_id
            uploaded.append(file_info)
            log(f"   ‚úÖ {file_info['name']} ‚Üí {new_doc_id[:20]}...")
        else:
            log(f"   ‚ùå {file_info['name']} - –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
    
    if not uploaded:
        log("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
        return
    
    # –®–∞–≥ 4: –ñ–¥—ë–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    log("\n‚è≥ –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
    
    indexed = []
    for file_info in uploaded:
        log(f"   ‚è≥ {file_info['name']}...")
        if wait_for_indexing(file_info['new_doc_id']):
            indexed.append(file_info)
            log(f"   ‚úÖ {file_info['name']} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")
        else:
            log(f"   ‚ö†Ô∏è  {file_info['name']} - —Ç–∞–π–º–∞—É—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    
    if not indexed:
        log("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ")
        return
    
    # –®–∞–≥ 5: –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞ (–∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ ID –Ω–∞ –Ω–æ–≤—ã–µ)
    log("\nü§ñ –®–∞–≥ 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π KB - –∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –Ω–æ–≤—ã–µ
    new_agent_kb = []
    old_doc_ids = []  # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    
    for doc in agent_kb:
        name = doc['name']
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        updated = next((f for f in indexed if f['name'] == name), None)
        
        if updated:
            # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
            new_agent_kb.append({
                'type': doc.get('type', 'text'),
                'name': name,
                'id': updated['new_doc_id'],
                'usage_mode': doc.get('usage_mode', 'auto')
            })
            if updated.get('old_doc_id'):
                old_doc_ids.append(updated['old_doc_id'])
            log(f"   üîÑ {name}: {updated.get('old_doc_id', 'new')[:15]}... ‚Üí {updated['new_doc_id'][:15]}...")
        else:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            new_agent_kb.append(doc)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞
    if update_agent_kb(new_agent_kb):
        log("   ‚úÖ –ê–≥–µ–Ω—Ç –æ–±–Ω–æ–≤–ª—ë–Ω")
    else:
        log("   ‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞")
        return
    
    # –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ KB
    if old_doc_ids:
        log("\nüóëÔ∏è  –®–∞–≥ 6: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π...")
        time.sleep(2)  # –î–∞—ë–º –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–≤—è–∑–∫—É
        
        for old_id in old_doc_ids:
            if delete_document(old_id):
                log(f"   ‚úÖ –£–¥–∞–ª—ë–Ω: {old_id[:20]}...")
            else:
                log(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª—ë–Ω: {old_id[:20]}...")
    
    # –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
    log("\nüíæ –®–∞–≥ 7: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è...")
    
    for file_info in indexed:
        state['quarters'][file_info['name']] = {
            'doc_id': file_info['new_doc_id'],
            'content_hash': file_info['hash'],
            'last_updated': datetime.now().isoformat()
        }
    
    save_state(state)
    
    # –ò—Ç–æ–≥–∏
    log("\n" + "=" * 60)
    log("üìä –ò–¢–û–ì–ò:")
    log(f"   üì§ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(uploaded)}")
    log(f"   ‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(indexed)}")
    log(f"   üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö: {len(old_doc_ids)}")
    log("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='ElevenLabs Sync v2')
    parser.add_argument('--dir', default='quarters', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è')
    parser.add_argument('--changed-files', type=str, help='–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')
    
    args = parser.parse_args()
    
    changed_files = None
    if args.changed_files:
        with open(args.changed_files, 'r') as f:
            changed_files = [line.strip() for line in f if line.strip()]
    
    sync_quarters(
        quarters_dir=args.dir,
        changed_files=changed_files,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()

