#!/usr/bin/env python3
"""
ElevenLabs Sync v2 - –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –∞–≥–µ–Ω—Ç–æ–º

–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
1. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å: conversation_config.agent.prompt.knowledge_base
2. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (stateless: –ø–æ metadata.size_bytes, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –ø–æ —Ö–µ—à—É —á–µ—Ä–µ–∑ /content)
3. –ó–∞–º–µ–Ω—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –Ω–∞ –Ω–æ–≤—ã–µ (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ—Ç)
4. –£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ KB –ø–æ—Å–ª–µ –æ—Ç–≤—è–∑–∫–∏ –æ—Ç –∞–≥–µ–Ω—Ç–∞
"""

import os
import sys
import json
import time
import hashlib
import argparse
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env –µ—Å–ª–∏ –µ—Å—Ç—å
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
API_KEY = os.environ.get('ELEVENLABS_API_KEY')
AGENT_ID = os.environ.get('ELEVENLABS_AGENT_ID')
BASE_URL = "https://api.elevenlabs.io/v1"
RAG_EMBEDDING_MODEL = os.environ.get("RAG_EMBEDDING_MODEL", "multilingual_e5_large_instruct")

# –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω–µ –æ–±–Ω–æ–≤–ª—è–µ–º)
PERMANENT_DOCS = {
    '–ú–ë–ê', '00-obschie-svedeniya', '01-obrazovatelnaya-infrastruktura',
    '02-parkinki-i-sport', '03-finansovye-uslugi', '03-empathy-enhancer',
    '05-sroki-sdachi-domov'
}

STATE_FILE = Path('./quarters_state.json')


def log(msg: str):
    """–í—ã–≤–æ–¥ —Å timestamp"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}", flush=True)


def get_headers() -> dict:
    return {"xi-api-key": API_KEY}


def load_state() -> dict:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    if STATE_FILE.exists():
        with open(STATE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"quarters": {}, "permanent_docs": {}}


def save_state(state: dict):
    """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
    state["last_update"] = datetime.now().isoformat()
    with open(STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def calculate_hash(file_path: str) -> str:
    """MD5 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    with open(file_path, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()


def calculate_hash_text(text: str) -> str:
    """MD5 —Ö–µ—à —Å—Ç—Ä–æ–∫–∏ (UTF-8)"""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def read_text_file(file_path: str) -> str:
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


def utf8_size_bytes(text: str) -> int:
    return len(text.encode("utf-8"))


def get_kb_document_info(doc_id: str) -> Optional[Dict]:
    """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ KB (GET /knowledge-base/{id})"""
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}"
    try:
        resp = requests.get(url, headers=get_headers(), timeout=60)
        if resp.status_code == 200:
            return resp.json()
        return None
    except Exception:
        return None


def get_kb_document_content(doc_id: str) -> Optional[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ KB (GET /knowledge-base/{id}/content)"""
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}/content"
    try:
        resp = requests.get(url, headers=get_headers(), timeout=(30, 180))
        if resp.status_code == 200:
            return resp.text
        return None
    except Exception:
        return None


def create_text_document(text: str, name: str) -> Optional[str]:
    """–°–æ–∑–¥–∞—Ç—å text –¥–æ–∫—É–º–µ–Ω—Ç (POST /knowledge-base/text)"""
    url = f"{BASE_URL}/convai/knowledge-base/text"
    payload = {"text": text, "name": name}
    try:
        resp = requests.post(
            url,
            headers={**get_headers(), "Content-Type": "application/json"},
            json=payload,
            timeout=(30, 180),
        )
        if resp.status_code in [200, 201]:
            data = resp.json()
            return data.get("id") or data.get("knowledge_base_id")
        return None
    except Exception:
        return None


def get_agent_kb() -> List[Dict]:
    """–ü–æ–ª—É—á–∏—Ç—å knowledge_base –∞–≥–µ–Ω—Ç–∞ (–ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!)"""
    url = f"{BASE_URL}/convai/agents/{AGENT_ID}"
    resp = requests.get(url, headers=get_headers(), timeout=60)
    
    if resp.status_code != 200:
        log(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞: {resp.status_code}")
        return []
    
    data = resp.json()
    # –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!
    kb = data.get('conversation_config', {}).get('agent', {}).get('prompt', {}).get('knowledge_base', [])
    return kb


def upload_document(file_path: str, name: str) -> Optional[str]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –≤ KB –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é
    
    –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º /knowledge-base/text (JSON), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å UTF-8 –∏ —Ç–∏–ø–æ–º file.
    """
    try:
        markdown_content = read_text_file(file_path)
        doc_id = create_text_document(text=markdown_content, name=name)
        if not doc_id:
            log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {name}: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å text –¥–æ–∫—É–º–µ–Ω—Ç")
            return None

        log(f"      üì§ –ó–∞–≥—Ä—É–∂–µ–Ω (text): {doc_id[:20]}...")

        # –í–ê–ñ–ù–û: –ó–∞–ø—É—Å–∫–∞–µ–º RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —è–≤–Ω–æ!
        trigger_rag_indexing(doc_id)
        return doc_id
            
    except Exception as e:
        log(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {name}: {e}")
        return None


def trigger_rag_indexing(doc_id: str) -> bool:
    """–Ø–≤–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    ElevenLabs –ù–ï –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ API!
    –ù—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å POST /convai/knowledge-base/{id}/rag-index
    """
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}/rag-index"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å —á—Ç–æ —É –∞–≥–µ–Ω—Ç–∞
    data = {
        "model": RAG_EMBEDDING_MODEL
    }
    
    try:
        resp = requests.post(
            url, 
            headers={**get_headers(), "Content-Type": "application/json"},
            json=data,
            timeout=60
        )
        
        if resp.status_code in [200, 201, 202]:
            # compute-rag-index –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–µ–Ω: –µ—Å–ª–∏ —É–∂–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω ‚Äî –≤–µ—Ä–Ω—ë—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
            log(f"      ‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—Ä–æ—à–µ–Ω–∞ (compute-rag-index)")
            return True
        else:
            log(f"      ‚ö†Ô∏è  –°—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {resp.status_code}")
            return False
            
    except Exception as e:
        log(f"      ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
        return False


def check_indexing_status(doc_id: str) -> str:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    ElevenLabs API –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
    {
        "indexes": [
            {"status": "succeeded", "progress_percentage": 100.0, ...}
        ]
    }
    """
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}/rag-index"
    
    try:
        resp = requests.get(url, headers=get_headers(), timeout=30)
        if resp.status_code == 200:
            data = resp.json()
            # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å: indexes[0].status
            indexes = data.get('indexes', [])
            if indexes:
                return indexes[0].get('status', 'unknown')
            return 'no_index'
        return 'error'
    except:
        return 'error'


def wait_for_indexing(doc_id: str, max_wait: int = 120) -> bool:
    """–î–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    Args:
        doc_id: ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        max_wait: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    
    Returns:
        True –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, False –µ—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç
    """
    start = time.time()
    
    while time.time() - start < max_wait:
        status = check_indexing_status(doc_id)
        
        # succeeded - —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω (ElevenLabs API)
        if status in ['succeeded']:
            return True
        elif status in ['failed', 'rag_limit_exceeded', 'document_too_small']:
            log(f"      ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {status}")
            return False
        
        # –ñ–¥—ë–º –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞
        time.sleep(5)
    
    log(f"      ‚ö†Ô∏è  –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    return False


def update_agent_kb(new_kb: List[Dict]) -> bool:
    """–û–±–Ω–æ–≤–∏—Ç—å KB –∞–≥–µ–Ω—Ç–∞ (–ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–£–¢–¨!)"""
    url = f"{BASE_URL}/convai/agents/{AGENT_ID}"
    
    update_data = {
        "conversation_config": {
            "agent": {
                "prompt": {
                    "knowledge_base": new_kb
                }
            }
        }
    }
    
    log(f"   üì§ PATCH –∑–∞–ø—Ä–æ—Å ({len(new_kb)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
    
    # Retry —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º —Ç–∞–π–º–∞—É—Ç–æ–º
    for attempt in range(3):
        try:
            resp = requests.patch(
                url,
                headers={**get_headers(), "Content-Type": "application/json"},
                json=update_data,
                timeout=(30, 600)  # 30s connect, 10min read (–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–æ–π)
            )
            
            if resp.status_code == 200:
                log(f"   ‚úÖ –ê–≥–µ–Ω—Ç –æ–±–Ω–æ–≤–ª—ë–Ω —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                log(f"   ‚ùå –û—à–∏–±–∫–∞: {resp.status_code} - {resp.text[:200]}")
                return False
                
        except requests.exceptions.Timeout:
            log(f"   ‚ö†Ô∏è  –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/3: —Ç–∞–π–º–∞—É—Ç")
            if attempt < 2:
                time.sleep(5)
        except Exception as e:
            log(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
            return False
    
    log("   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
    return False


def delete_document(doc_id: str) -> bool:
    """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ KB"""
    url = f"{BASE_URL}/convai/knowledge-base/{doc_id}"
    try:
        resp = requests.delete(url, headers=get_headers(), timeout=30)
        if resp.status_code in [200, 204]:
            return True
        # fallback: force delete
        resp2 = requests.delete(f"{url}?force=true", headers=get_headers(), timeout=30)
        return resp2.status_code in [200, 204]
    except Exception:
        return False


def init_state_from_agent(agent_docs: dict, quarters_path: Path) -> dict:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å state –∏–∑ —Ç–µ–∫—É—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞"""
    state = {"quarters": {}, "permanent_docs": {}}
    
    for name, doc in agent_docs.items():
        md_file = quarters_path / f"{name}.md"
        content_hash = ""
        if md_file.exists():
            content_hash = calculate_hash(str(md_file))
        
        doc_info = {
            "doc_id": doc.get('id', ''),
            "content_hash": content_hash,
            "last_updated": None
        }
        
        if name in PERMANENT_DOCS:
            state["permanent_docs"][name] = doc_info
        else:
            state["quarters"][name] = doc_info
    
    return state


def should_update_doc_stateless(local_text: str, existing_doc_id: Optional[str], strict_hash: bool = False) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –±–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ state.

    - –ë—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å: —Å—Ä–∞–≤–Ω–∏—Ç—å local_size (utf-8) —Å KB metadata.size_bytes
    - –ï—Å–ª–∏ strict_hash=True –∏ size_bytes —Ä–∞–≤–Ω—ã: –¥–æ–∫–∞—á–∞—Ç—å /content –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ö–µ—à
    """
    if not existing_doc_id:
        return True

    local_size = utf8_size_bytes(local_text)
    info = get_kb_document_info(existing_doc_id)
    if info:
        kb_size = (info.get("metadata") or {}).get("size_bytes")
        if isinstance(kb_size, int):
            if kb_size != local_size:
                return True
        else:
            # –µ—Å–ª–∏ metadata.size_bytes –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî fallback –Ω–∞ /content
            kb_text = get_kb_document_content(existing_doc_id)
            if kb_text is None:
                return True
            return calculate_hash_text(kb_text) != calculate_hash_text(local_text)
    else:
        # –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ ‚Äî fallback –Ω–∞ /content
        kb_text = get_kb_document_content(existing_doc_id)
        if kb_text is None:
            return True
        return calculate_hash_text(kb_text) != calculate_hash_text(local_text)

    if strict_hash:
        kb_text = get_kb_document_content(existing_doc_id)
        if kb_text is None:
            # –µ—Å–ª–∏ –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç - –±–µ–∑–æ–ø–∞—Å–Ω–µ–µ –æ–±–Ω–æ–≤–∏—Ç—å, —á–µ–º –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
            return True
        return calculate_hash_text(kb_text) != calculate_hash_text(local_text)

    return False


def sync_quarters(
    quarters_dir: str = 'quarters',
    changed_files: List[str] = None,
    dry_run: bool = False,
    strict_hash: bool = False,
    index_wait: int = 120,
):
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    
    Args:
        quarters_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏
        changed_files: –°–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        dry_run: –¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
        strict_hash: –ü—Ä–∏ —Ä–∞–≤–Ω–æ–º size_bytes —Å–≤–µ—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ /content
        index_wait: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (—Å–µ–∫)
    """
    log("=" * 60)
    log("üöÄ ElevenLabs Sync v2")
    log("=" * 60)
    
    if not API_KEY or not AGENT_ID:
        log("‚ùå –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ELEVENLABS_API_KEY –∏ ELEVENLABS_AGENT_ID")
        return
    
    quarters_path = Path(quarters_dir)
    
    # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∞–≥–µ–Ω—Ç–∞
    log("\nüì• –®–∞–≥ 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞...")
    agent_kb = get_agent_kb()
    log(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∞–≥–µ–Ω—Ç–µ: {len(agent_kb)}")
    
    # –°–æ–∑–¥–∞—ë–º —Å–ª–æ–≤–∞—Ä—å name ‚Üí doc –¥–ª—è –∞–≥–µ–Ω—Ç–∞
    agent_docs = {doc['name']: doc for doc in agent_kb}
    
    # –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
    log("\nüîç –®–∞–≥ 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π...")
    
    files_to_update = []
    files_to_update_names = set()  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
    if changed_files:
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ changed_files
        unique_files = list(dict.fromkeys(changed_files))
        md_files = [quarters_path / f for f in unique_files if f.endswith('.md')]
    else:
        md_files = list(quarters_path.glob('*.md'))
    
    for md_file in md_files:
        name = md_file.stem  # –ò–º—è –±–µ–∑ .md
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if name in PERMANENT_DOCS:
            continue
        
        local_text = read_text_file(str(md_file))
        existing_doc_id = agent_docs.get(name, {}).get('id')

        # Stateless —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å KB
        if should_update_doc_stateless(local_text, existing_doc_id, strict_hash=strict_hash) and name not in files_to_update_names:
            files_to_update_names.add(name)
            files_to_update.append({
                'name': name,
                'path': str(md_file),
                'hash': calculate_hash_text(local_text),
                'old_doc_id': existing_doc_id
            })
            if existing_doc_id:
                log(f"   üîÑ {name} (–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)")
            else:
                log(f"   ‚ûï {name} (–Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç)")
        else:
            log(f"   ‚úÖ {name} (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
    
    if not files_to_update:
        log("\n‚úÖ –ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏")
        return
    
    log(f"\nüìä –ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—é: {len(files_to_update)} —Ñ–∞–π–ª–æ–≤")
    
    if dry_run:
        log("\n‚ö†Ô∏è  DRY RUN - –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã")
        return
    
    # –®–∞–≥ 3: –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –≤–µ—Ä—Å–∏–∏
    log("\nüì§ –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –≤–µ—Ä—Å–∏–π...")
    
    uploaded = []
    for file_info in files_to_update:
        log(f"   üì§ {file_info['name']}...", )
        
        new_doc_id = upload_document(file_info['path'], file_info['name'])
        if new_doc_id:
            file_info['new_doc_id'] = new_doc_id
            uploaded.append(file_info)
            log(f"   ‚úÖ {file_info['name']} ‚Üí {new_doc_id[:20]}...")
        else:
            log(f"   ‚ùå {file_info['name']} - –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
    
    if not uploaded:
        log("‚ùå –ù–∏—á–µ–≥–æ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
        return
    
    # –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    log("\n‚è≥ –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
    
    indexed = []
    for file_info in uploaded:
        doc_id = file_info['new_doc_id']
        name = file_info['name']
        
        log(f"   üîç {name}...", )
        
        # –ñ–¥—ë–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 120 —Å–µ–∫—É–Ω–¥)
        if wait_for_indexing(doc_id, max_wait=index_wait):
            indexed.append(file_info)
            log(f"   ‚úÖ {name} –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")
        else:
            # –î–∞–∂–µ –µ—Å–ª–∏ –Ω–µ –¥–æ–∂–¥–∞–ª–∏—Å—å - –¥–æ–±–∞–≤–ª—è–µ–º, –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—Å—è –≤ —Ñ–æ–Ω–µ
            indexed.append(file_info)
            log(f"   ‚ö†Ô∏è  {name} - –∏–Ω–¥–µ–∫—Å –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω (–ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–∑–∂–µ)")
    
    log(f"   üìä –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(indexed)}")
    
    # –®–∞–≥ 5: –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞ (–∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ ID –Ω–∞ –Ω–æ–≤—ã–µ)
    log("\nü§ñ –®–∞–≥ 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
    
    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π KB - –∑–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –Ω–æ–≤—ã–µ
    new_agent_kb = []
    old_doc_ids = []  # –î–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    seen_names = set()  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    
    for doc in agent_kb:
        name = doc['name']
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π)
        if name in seen_names:
            log(f"   ‚ö†Ô∏è  {name}: –¥—É–±–ª–∏–∫–∞—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            old_doc_ids.append(doc['id'])  # –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç
            continue
        seen_names.add(name)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        updated = next((f for f in indexed if f['name'] == name), None)
        
        if updated:
            # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
            new_agent_kb.append({
                'type': doc.get('type', 'text'),
                'name': name,
                'id': updated['new_doc_id'],
                'usage_mode': doc.get('usage_mode', 'auto')
            })
            if updated.get('old_doc_id'):
                old_doc_ids.append(updated['old_doc_id'])
            log(f"   üîÑ {name}: {updated.get('old_doc_id', 'new')[:15]}... ‚Üí {updated['new_doc_id'][:15]}...")
        else:
            # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
            new_agent_kb.append(doc)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–Ω–µ–µ –Ω–µ –±—ã–ª–æ –≤ –∞–≥–µ–Ω—Ç–µ
    existing_names = {d.get("name") for d in new_agent_kb}
    for upd in indexed:
        if upd["name"] not in existing_names:
            new_agent_kb.append({
                "type": "text",
                "name": upd["name"],
                "id": upd["new_doc_id"],
                "usage_mode": "auto",
            })
            existing_names.add(upd["name"])
            log(f"   ‚ûï –î–æ–±–∞–≤–ª–µ–Ω –≤ –∞–≥–µ–Ω—Ç–∞: {upd['name']}")
    
    log(f"   üìä –ò—Ç–æ–≥–æ –≤ –∞–≥–µ–Ω—Ç–µ: {len(new_agent_kb)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞
    if not update_agent_kb(new_agent_kb):
        return
    
    # –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –∏–∑ KB
    if old_doc_ids:
        log("\nüóëÔ∏è  –®–∞–≥ 6: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π...")
        time.sleep(2)  # –î–∞—ë–º –≤—Ä–µ–º—è –Ω–∞ –æ—Ç–≤—è–∑–∫—É
        
        for old_id in old_doc_ids:
            if delete_document(old_id):
                log(f"   ‚úÖ –£–¥–∞–ª—ë–Ω: {old_id[:20]}...")
            else:
                log(f"   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª—ë–Ω: {old_id[:20]}...")
    
    # –ò—Ç–æ–≥–∏
    log("\n" + "=" * 60)
    log("üìä –ò–¢–û–ì–ò:")
    log(f"   üì§ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {len(uploaded)}")
    log(f"   ‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(indexed)}")
    log(f"   üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö: {len(old_doc_ids)}")
    log("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='ElevenLabs Sync v2')
    parser.add_argument('--dir', default='quarters', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è')
    parser.add_argument('--changed-files', type=str, help='–§–∞–π–ª —Å–æ —Å–ø–∏—Å–∫–æ–º –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')
    parser.add_argument('--strict-hash', action='store_true', help='–ü—Ä–∏ —Ä–∞–≤–Ω–æ–º size_bytes —Å–≤–µ—Ä—è—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —á–µ—Ä–µ–∑ /content')
    parser.add_argument('--index-wait', type=int, default=int(os.environ.get("RAG_INDEXING_TIMEOUT", "120")), help='–û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (—Å–µ–∫)')
    
    args = parser.parse_args()
    
    changed_files = None
    if args.changed_files:
        with open(args.changed_files, 'r') as f:
            changed_files = [line.strip() for line in f if line.strip()]
    
    sync_quarters(
        quarters_dir=args.dir,
        changed_files=changed_files,
        dry_run=args.dry_run,
        strict_hash=args.strict_hash,
        index_wait=args.index_wait,
    )


if __name__ == "__main__":
    main()

