#!/usr/bin/env python3
"""
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Ä–∞–∑–æ–≤–æ–π —á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ElevenLabs Knowledge Base

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 cleanup_old_kb_docs.py --dry-run    # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ
    python3 cleanup_old_kb_docs.py --confirm    # –í—ã–ø–æ–ª–Ω–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ
"""

import os
import json
import requests
import re
from datetime import datetime
from typing import List, Dict
from dotenv import load_dotenv
from collections import defaultdict

load_dotenv()


class KnowledgeBaseCleanup:
    """–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""

    def __init__(self):
        self.api_key = os.getenv('ELEVENLABS_API_KEY')

        if not self.api_key:
            raise ValueError("‚ùå ELEVENLABS_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")

        self.base_url = "https://api.elevenlabs.io/v1"
        self.headers = {"xi-api-key": self.api_key}

    def get_all_documents(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Knowledge Base"""
        all_docs = []
        page = 0

        print("üìö –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        while True:
            url = f"{self.base_url}/convai/knowledge-base?page_size=100&page={page}"

            try:
                response = requests.get(url, headers=self.headers, timeout=30)

                if response.status_code != 200:
                    break

                data = response.json()
                docs = data.get('documents', data.get('knowledge_bases', []))

                if not docs:
                    break

                all_docs.extend(docs)

                if not data.get('has_more', False):
                    break

                page += 1

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
                break

        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(all_docs)}\n")
        return all_docs

    def analyze_versions(self, all_docs: List[Dict]) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Ä—Å–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            {
                'to_delete': [...],  # –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
                'to_keep': [...],    # –î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                'stats': {...}       # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            }
        """
        grouped = defaultdict(list)

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –±–∞–∑–æ–≤–æ–º—É –∏–º–µ–Ω–∏
        for doc in all_docs:
            name = doc.get('name', '')

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ
            if any(x in name.lower() for x in ['system', 'elevenlabs_rag', 'prompt', 'instructions']):
                continue

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è
            base_name = re.sub(r'-v\d+|-\d{4}-\d{2}-\d{2}', '', name)
            base_name = re.sub(r'\.(txt|md|html)$', '', base_name)

            grouped[base_name].append(doc)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        to_delete = []
        to_keep = []
        multiple_versions = {}

        for base_name, versions in grouped.items():
            if len(versions) == 1:
                to_keep.extend(versions)
            else:
                # –ï—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–µ—Ä—Å–∏–π
                multiple_versions[base_name] = len(versions)

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
                versions_sorted = sorted(
                    versions,
                    key=lambda x: x.get('metadata', {}).get('created_at_unix_secs', 0),
                    reverse=True
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∞–º—É—é –Ω–æ–≤—É—é
                to_keep.append(versions_sorted[0])

                # –û—Å—Ç–∞–ª—å–Ω—ã–µ - —É–¥–∞–ª—è–µ–º
                for old_version in versions_sorted[1:]:
                    to_delete.append(old_version)

        stats = {
            'total_documents': len(all_docs),
            'unique_base_names': len(grouped),
            'documents_with_multiple_versions': len(multiple_versions),
            'versions_to_delete': len(to_delete),
            'versions_to_keep': len(to_keep),
            'multiple_versions_detail': multiple_versions
        }

        return {
            'to_delete': to_delete,
            'to_keep': to_keep,
            'stats': stats
        }

    def print_analysis(self, analysis: Dict):
        """–í—ã–≤–µ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"""
        stats = analysis['stats']
        to_delete = analysis['to_delete']

        print("=" * 70)
        print("üìä –ê–ù–ê–õ–ò–ó –í–ï–†–°–ò–ô –î–û–ö–£–ú–ï–ù–¢–û–í")
        print("=" * 70)
        print(f"–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {stats['total_documents']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –≤–µ—Ä—Å–∏–π): {stats['unique_base_names']}")
        print(f"–§–∞–π–ª–æ–≤ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏: {stats['documents_with_multiple_versions']}")
        print(f"–í–µ—Ä—Å–∏–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {stats['versions_to_delete']}")
        print(f"–í–µ—Ä—Å–∏–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {stats['versions_to_keep']}")

        if stats['multiple_versions_detail']:
            print("\nüìã –§–∞–π–ª—ã —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏:")
            for base_name, count in sorted(
                stats['multiple_versions_detail'].items(),
                key=lambda x: x[1],
                reverse=True
            )[:20]:
                print(f"   {base_name[:50]:50} - {count} –≤–µ—Ä—Å–∏–π")

            if len(stats['multiple_versions_detail']) > 20:
                print(f"   ... –∏ –µ—â–µ {len(stats['multiple_versions_detail']) - 20}")

        if to_delete:
            print(f"\nüóëÔ∏è  –î–û–ö–£–ú–ï–ù–¢–´ –î–õ–Ø –£–î–ê–õ–ï–ù–ò–Ø (–≤—Å–µ–≥–æ {len(to_delete)}):")
            print(f"   –ü–µ—Ä–≤—ã–µ 20:")
            for i, doc in enumerate(to_delete[:20], 1):
                name = doc.get('name', 'Unknown')
                doc_id = doc.get('id', '')
                created = doc.get('metadata', {}).get('created_at_unix_secs', 0)

                if created:
                    date_str = datetime.fromtimestamp(created).strftime('%Y-%m-%d')
                else:
                    date_str = 'N/A'

                print(f"   {i:2}. {name[:50]:50} | {date_str} | {doc_id[:15]}...")

            if len(to_delete) > 20:
                print(f"   ... –∏ –µ—â–µ {len(to_delete) - 20} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

        print("=" * 70)

    def delete_documents(self, to_delete: List[Dict]) -> int:
        """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        deleted_count = 0

        print(f"\nüóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ {len(to_delete)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        for i, doc in enumerate(to_delete, 1):
            doc_id = doc.get('id')
            name = doc.get('name', 'Unknown')

            print(f"  {i}/{len(to_delete)} - {name[:50]:50}", end=" ")

            url = f"{self.base_url}/convai/knowledge-base/{doc_id}"

            try:
                response = requests.delete(url, headers=self.headers, timeout=30)

                if response.status_code in [200, 204]:
                    deleted_count += 1
                    print("‚úÖ")
                else:
                    print(f"‚ùå (–∫–æ–¥ {response.status_code})")

            except Exception as e:
                print(f"‚ùå ({str(e)[:30]})")

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —É–¥–∞–ª–µ–Ω–∏—è–º–∏
            if i < len(to_delete):
                import time
                time.sleep(0.5)

        return deleted_count

    def run(self, dry_run: bool = True):
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å —á–∏—Å—Ç–∫—É

        Args:
            dry_run: –ï—Å–ª–∏ True, —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –∞–Ω–∞–ª–∏–∑ –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è
        """
        print("=" * 70)
        print("üßπ –£–¢–ò–õ–ò–¢–ê –ß–ò–°–¢–ö–ò KNOWLEDGE BASE")
        print("=" * 70)
        print()

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        all_docs = self.get_all_documents()

        if not all_docs:
            print("‚ÑπÔ∏è  –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
        analysis = self.analyze_versions(all_docs)

        # –í—ã–≤–æ–¥–∏–º –∞–Ω–∞–ª–∏–∑
        self.print_analysis(analysis)

        # –í—ã–ø–æ–ª–Ω—è–µ–º —É–¥–∞–ª–µ–Ω–∏–µ
        to_delete = analysis['to_delete']

        if not to_delete:
            print("\n‚úÖ –°—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —á–∏—Å—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è!")
            return

        if dry_run:
            print("\n‚ö†Ô∏è  –†–ï–ñ–ò–ú DRY-RUN: –î–æ–∫—É–º–µ–Ω—Ç—ã –ù–ï –±—É–¥—É—Ç —É–¥–∞–ª–µ–Ω—ã")
            print("   –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å —Ñ–ª–∞–≥–æ–º --confirm")
        else:
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
            print(f"\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï! –ë—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ {len(to_delete)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤!")
            print("   –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ–æ–±—Ä–∞—Ç–∏–º–æ!")

            response = input("\n   –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å? (yes/no): ").strip().lower()

            if response != 'yes':
                print("\n‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                return

            # –£–¥–∞–ª—è–µ–º
            deleted_count = self.delete_documents(to_delete)

            print("\n" + "=" * 70)
            print(f"‚úÖ –ß–ò–°–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê!")
            print(f"   –£–¥–∞–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {deleted_count}/{len(to_delete)}")
            print("=" * 70)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description='–ß–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ ElevenLabs Knowledge Base',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  %(prog)s --dry-run    # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ (–±–µ–∑–æ–ø–∞—Å–Ω–æ)
  %(prog)s --confirm    # –í—ã–ø–æ–ª–Ω–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ (—Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è)
        """
    )
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑, –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è')
    group.add_argument('--confirm', action='store_true', help='–í—ã–ø–æ–ª–Ω–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º')

    args = parser.parse_args()

    try:
        cleanup = KnowledgeBaseCleanup()
        cleanup.run(dry_run=args.dry_run)
        return 0
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
