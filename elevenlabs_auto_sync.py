#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ElevenLabs —Å —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 elevenlabs_auto_sync.py              # –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
    python3 elevenlabs_auto_sync.py --dry-run    # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
    python3 elevenlabs_auto_sync.py --no-delete  # –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∞, –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è
"""

import os
import json
import requests
import time
import hashlib
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from dotenv import load_dotenv
from collections import defaultdict
import sys

load_dotenv()


def log(msg):
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}", flush=True)


class ElevenLabsAutoSync:
    """–ü–æ–ª–Ω–∞—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ Knowledge Base —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≤–µ—Ä—Å–∏—è–º–∏"""

    def __init__(self):
        self.api_key = os.getenv('ELEVENLABS_API_KEY')
        self.agent_id = os.getenv('ELEVENLABS_AGENT_ID')

        if not self.api_key:
            raise ValueError("‚ùå ELEVENLABS_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
        if not self.agent_id:
            raise ValueError("‚ùå ELEVENLABS_AGENT_ID –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")

        self.base_url = "https://api.elevenlabs.io/v1"
        self.headers = {
            "xi-api-key": self.api_key,
            "Content-Type": "application/json"
        }

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π
        self.log_file = "elevenlabs_sync_log.json"
        self.load_sync_log()

    def load_sync_log(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–π"""
        if os.path.exists(self.log_file):
            with open(self.log_file, 'r', encoding='utf-8') as f:
                self.sync_log = json.load(f)
        else:
            self.sync_log = {
                "last_sync": None,
                "uploads": {},  # filename -> {id, upload_date, hash}
                "deletions": []  # [{id, name, deletion_date, reason}]
            }

    def save_sync_log(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–π"""
        with open(self.log_file, 'w', encoding='utf-8') as f:
            json.dump(self.sync_log, f, indent=2, ensure_ascii=False)

    def get_all_kb_documents_cached(self, ttl_minutes: int = 60, use_cache_only: bool = False) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å KB –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö API –≤—ã–∑–æ–≤–æ–≤

        Args:
            ttl_minutes: –í—Ä–µ–º—è –∂–∏–∑–Ω–∏ –∫—ç—à–∞ –≤ –º–∏–Ω—É—Ç–∞—Ö
            use_cache_only: –ï—Å–ª–∏ True, –Ω–µ –¥–µ–ª–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∫ API, —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à

        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ KB
        """
        cache_file = Path('.elevenlabs_kb_cache.json')

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∫—ç—à–∞
        if cache_file.exists():
            mtime = cache_file.stat().st_mtime
            age_minutes = (time.time() - mtime) / 60

            if age_minutes < ttl_minutes:
                log(f"  üì¶ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∞ KB (–≤–æ–∑—Ä–∞—Å—Ç: {age_minutes:.1f} –º–∏–Ω)")
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached_docs = json.load(f)
                        log(f"  ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞: {len(cached_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                        return cached_docs
                except Exception as e:
                    log(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞: {e}, –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ")

        # –ï—Å–ª–∏ use_cache_only=True –∏ –∫—ç—à–∞ –Ω–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        if use_cache_only:
            log(f"  ‚ö†Ô∏è  –ö—ç—à –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª, –Ω–æ —Ä–µ–∂–∏–º use_cache_only=True")
            log(f"  üí° –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π (—Ç–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö)")
            return []

        # –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ
        log(f"  üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö KB...")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ä—ã–π –∫—ç—à –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏
        old_cache_docs = []
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    old_cache_docs = json.load(f)
                    log(f"  üì¶ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Å—Ç–∞—Ä—ã–π –∫—ç—à –¥–ª—è fallback ({len(old_cache_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
            except:
                pass
        
        try:
            docs = self.get_all_kb_documents()

            # –ö—ç—à–∏—Ä—É–µ–º
            log(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ ({len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)...")
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(docs, f, ensure_ascii=False, indent=2)
                log(f"  ‚úÖ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
            except Exception as e:
                log(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")

            return docs
        except Exception as e:
            log(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {type(e).__name__} - {str(e)[:200]}")
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–∞—Ä—ã–π –∫—ç—à - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ fallback
            if old_cache_docs:
                log(f"  üîÑ Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –∫—ç—à ({len(old_cache_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
                log(f"  ‚ö†Ô∏è  –í–Ω–∏–º–∞–Ω–∏–µ: —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º, —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –º–æ–≥—É—Ç –Ω–µ —É–¥–∞–ª–∏—Ç—å—Å—è")
                return old_cache_docs
            
            log(f"  üí° –°—Ç–∞—Ä–æ–≥–æ –∫—ç—à–∞ –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
            return []

    # ===== –ü–û–õ–£–ß–ï–ù–ò–ï –°–ü–ò–°–ö–ê –î–û–ö–£–ú–ï–ù–¢–û–í =====

    def get_all_kb_documents(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Knowledge Base —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        all_docs = []
        page = 0
        max_pages = 100  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞

        log(f"   üì• –ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ KB (—Ç–∞–π–º–∞—É—Ç: 60—Å)")
        
        while page < max_pages:
            url = f"{self.base_url}/convai/knowledge-base?page_size=100&page={page}"
            
            try:
                log(f"   üìÑ –ó–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}...")
                response = requests.get(url, headers={"xi-api-key": self.api_key}, timeout=60)

                if response.status_code != 200:
                    log(f"   ‚ùå HTTP {response.status_code} –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}")
                    if page == 0:
                        log(f"   üìù –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.text[:500]}")
                    break

                data = response.json()
                docs = data.get('documents', data.get('knowledge_bases', []))

                if not docs:
                    log(f"   ‚ÑπÔ∏è  –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ")
                    break

                all_docs.extend(docs)
                log(f"   ‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: –ø–æ–ª—É—á–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–≤—Å–µ–≥–æ: {len(all_docs)})")

                has_more = data.get('has_more', False)
                if not has_more:
                    log(f"   ‚úÖ –í—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã (has_more=False)")
                    break

                page += 1
                time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            except requests.exceptions.Timeout:
                log(f"   ‚ùå –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} (>60 —Å–µ–∫)")
                break
            except requests.exceptions.RequestException as e:
                log(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {type(e).__name__} - {str(e)[:200]}")
                break
            except Exception as e:
                log(f"   ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page}: {type(e).__name__} - {str(e)[:200]}")
                break

        log(f"   üìä –ò—Ç–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(all_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {page + 1} —Å—Ç—Ä–∞–Ω–∏—Ü")
        return all_docs

    # ===== –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–û–ö–£–ú–ï–ù–¢–û–í –î–õ–Ø –£–î–ê–õ–ï–ù–ò–Ø =====

    def identify_documents_to_delete(self, all_docs: List[Dict], changed_files: List[str] = None) -> List[Dict]:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (—Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏)

        Args:
            all_docs: –í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ KB
            changed_files: –°–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (—Ç–æ–ª—å–∫–æ –∏—Ö —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ —É–¥–∞–ª—è–µ–º)

        –õ–æ–≥–∏–∫–∞:
        - –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –±–∞–∑–æ–≤–æ–º—É –∏–º–µ–Ω–∏ (–±–µ–∑ –¥–∞—Ç –∏ –≤–µ—Ä—Å–∏–π)
        - –í –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø–µ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–π –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
        - –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–º–µ—á–∞–µ–º –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        - –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω changed_files, —É–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ —ç—Ç–∏—Ö —Ñ–∞–π–ª–æ–≤
        """
        to_delete = []
        grouped = defaultdict(list)

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –∏–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∏–º–µ–Ω–∞
        changed_base_names = set()
        if changed_files:
            for f in changed_files:
                # –£–±–∏—Ä–∞–µ–º .md —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                base = f.replace('.md', '')
                changed_base_names.add(base)

        for doc in all_docs:
            name = doc.get('name', '')

            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            if any(x in name.lower() for x in ['system', 'elevenlabs_rag', 'prompt']):
                continue

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–∞–∑–æ–≤–æ–µ –∏–º—è (—É–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã –∏ –≤–µ—Ä—Å–∏–∏)
            # –ü—Ä–∏–º–µ—Ä—ã: "02-emirats-2025-11-20" -> "02-emirats"
            #          "10-Tropicheskie-ostrova-v2" -> "10-Tropicheskie-ostrova"
            base_name = re.sub(r'-v\d+|-\d{4}-\d{2}-\d{2}', '', name)
            base_name = re.sub(r'\.(txt|md)$', '', base_name)

            grouped[base_name].append(doc)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –≥—Ä—É–ø–ø—É
        for base_name, versions in grouped.items():
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä changed_files, –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
            if changed_files and base_name not in changed_base_names:
                # –≠—Ç–æ—Ç —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
            
            # –õ–æ–≥–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if changed_files:
                log(f"   üîç –ê–Ω–∞–ª–∏–∑: {base_name} ({len(versions)} –≤–µ—Ä—Å–∏–π)")
            
            if len(versions) <= 1:
                continue  # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ –≤–µ—Ä—Å–∏—è - –Ω–∏—á–µ–≥–æ –Ω–µ —É–¥–∞–ª—è–µ–º

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ —Å–æ–∑–¥–∞–Ω–∏—è
            versions_with_date = []

            for v in versions:
                doc_id = v.get('id')
                doc_name = v.get('name', 'unknown')
                
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –¥–∞—Ç—É —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ElevenLabs
                metadata = v.get('metadata', {})
                created = metadata.get('created_at_unix_secs')
                
                if created:
                    versions_with_date.append((v, created))
                    if changed_files:
                        log(f"      ‚úÖ –î–∞—Ç–∞ –∏–∑ metadata: {doc_name[:40]} -> {datetime.fromtimestamp(created).strftime('%Y-%m-%d %H:%M:%S')}")
                else:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ª–æ–≥ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–π
                    found_in_log = False
                    for log_name, log_info in self.sync_log['uploads'].items():
                        if log_info.get('id') == doc_id:
                            upload_date = log_info.get('upload_date', '')
                            if upload_date:
                                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º ISO –¥–∞—Ç—É –≤ unix timestamp
                                try:
                                    dt = datetime.fromisoformat(upload_date.replace('Z', '+00:00'))
                                    timestamp = int(dt.timestamp())
                                    versions_with_date.append((v, timestamp))
                                    found_in_log = True
                                    if changed_files:
                                        log(f"      ‚úÖ –î–∞—Ç–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∞: {doc_name[:40]} -> {upload_date}")
                                    break
                                except Exception as e:
                                    if changed_files:
                                        log(f"      ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–∞—Ç—ã –∏–∑ –ª–æ–≥–∞: {e}")
                    
                    if not found_in_log and changed_files:
                        log(f"      ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –¥–ª—è: {doc_name[:40]} (ID: {doc_id[:20]}...)")
                        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –µ—Å—Ç—å –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        if metadata:
                            log(f"         –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {json.dumps(metadata, ensure_ascii=False)[:200]}")

            if not versions_with_date:
                if changed_files:
                    log(f"      ‚ö†Ô∏è  –ü—Ä–æ–ø—É—Å–∫ {base_name}: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞—Ç—ã –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–π –≤–µ—Ä—Å–∏–∏")
                continue  # –ù–µ —Å–º–æ–≥–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –¥–∞—Ç—ã

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
            versions_with_date.sort(key=lambda x: x[1], reverse=True)

            # –í—Å–µ –∫—Ä–æ–º–µ –ø–µ—Ä–≤–æ–≥–æ (—Å–∞–º–æ–≥–æ –Ω–æ–≤–æ–≥–æ) - —É–¥–∞–ª—è–µ–º
            for doc, date in versions_with_date[1:]:
                newest_date = datetime.fromtimestamp(versions_with_date[0][1]).strftime('%Y-%m-%d')
                doc_name = doc.get('name', 'unknown')
                
                to_delete.append({
                    'id': doc.get('id'),
                    'name': doc_name,
                    'reason': f'—Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è (–Ω–æ–≤–µ–π—à–∞—è –æ—Ç {newest_date})'
                })
                
                if changed_files:
                    log(f"      üóëÔ∏è  –ù–∞–π–¥–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è: {doc_name[:50]}")

        return to_delete

    # ===== –ó–ê–ì–†–£–ó–ö–ê –ù–û–í–´–• –î–û–ö–£–ú–ï–ù–¢–û–í =====

    def upload_new_documents(self, files_dir: str = 'quarters', changed_files: List[str] = None) -> List[str]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

        Args:
            files_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD-—Ñ–∞–π–ª–∞–º–∏
            changed_files: –°–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

        Returns:
            –°–ø–∏—Å–æ–∫ ID –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        uploaded_ids = []
        files_to_upload = []

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ MD —Ñ–∞–π–ª—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        if changed_files:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —É–∫–∞–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
            print(f"üéØ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {len(changed_files)} —Ñ–∞–π–ª–æ–≤")
            md_files_to_check = [Path(files_dir) / f for f in changed_files if f.endswith('.md')]
        else:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
            print(f"üìö –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö MD-—Ñ–∞–π–ª–æ–≤")
            md_files_to_check = list(Path(files_dir).glob('*.md'))

        for md_file in md_files_to_check:
            file_path = str(md_file)
            file_name = md_file.stem  # –ò–º—è –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

            # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞
            with open(file_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–≥—Ä—É–∂–∞—Ç—å
            should_upload = False

            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω —Å–ø–∏—Å–æ–∫ changed_files, —Ç–æ –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ –Ω–µ–≥–æ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∑–∞–≥—Ä—É–∂–µ–Ω—ã
            if changed_files:
                should_upload = True
                print(f"üéØ –§–∞–π–ª –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {file_name}.md")
            elif file_name not in self.sync_log['uploads']:
                # –ù–æ–≤—ã–π —Ñ–∞–π–ª
                should_upload = True
                print(f"üìÑ –ù–æ–≤—ã–π —Ñ–∞–π–ª: {file_name}.md")
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª
                old_hash = self.sync_log['uploads'][file_name].get('hash', '')
                if old_hash != file_hash:
                    should_upload = True
                    print(f"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {file_name}.md")

            if should_upload:
                files_to_upload.append({
                    'path': file_path,
                    'name': file_name,
                    'hash': file_hash
                })

        if not files_to_upload:
            print("‚ÑπÔ∏è  –ù–µ—Ç –Ω–æ–≤—ã—Ö –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return []

        print(f"\nüì§ –ó–∞–≥—Ä—É–∑–∫–∞ {len(files_to_upload)} —Ñ–∞–π–ª–æ–≤...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
        for i, file_info in enumerate(files_to_upload, 1):
            print(f"  {i}/{len(files_to_upload)} - {file_info['name']}.md", end=" ")

            doc_id = self._upload_single_document(
                file_info['path'],
                file_info['name']
            )

            if doc_id:
                uploaded_ids.append(doc_id)

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–≥
                self.sync_log['uploads'][file_info['name']] = {
                    'id': doc_id,
                    'upload_date': datetime.now().isoformat(),
                    'hash': file_info['hash']
                }

                self.save_sync_log()
                print(f"‚úÖ")
            else:
                print(f"‚ùå")

            time.sleep(0.5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∑–∫–∞–º–∏

        return uploaded_ids

    def _upload_single_document(self, file_path: str, doc_name: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç"""
        url = f"{self.base_url}/convai/knowledge-base"

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                markdown_content = f.read()

            # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π HTML —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º UTF-8
            html_wrapper = f'''<!DOCTYPE html>
<html>
<head><meta charset="UTF-8"></head>
<body><pre>{markdown_content}</pre></body>
</html>'''

            # –î–æ–±–∞–≤–ª—è–µ–º UTF-8 BOM –¥–ª—è —è–≤–Ω–æ–≥–æ —É–∫–∞–∑–∞–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏
            content_bytes = '\ufeff'.encode('utf-8') + html_wrapper.encode('utf-8')

            files = {
                'file': (f'{doc_name}.html', content_bytes, 'text/html')
            }
            data = {'name': doc_name}

            headers_upload = {"xi-api-key": self.api_key}

            response = requests.post(url, headers=headers_upload, files=files, data=data, timeout=60)

            if response.status_code in [200, 201]:
                result = response.json()
                doc_id = result.get('knowledge_base_id', result.get('id'))
                return doc_id
            else:
                print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {doc_name}: HTTP {response.status_code}")
                print(f"   –û—Ç–≤–µ—Ç: {response.text[:200]}")
                return None

        except Exception as e:
            print(f"\n‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {doc_name}: {str(e)}")
            return None

    # ===== –û–ñ–ò–î–ê–ù–ò–ï –ò–ù–î–ï–ö–°–ê–¶–ò–ò =====

    def wait_for_indexing(self, doc_ids: List[str], max_wait: int = 120) -> List[str]:
        """
        –ñ–¥–∞—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Returns:
            –°–ø–∏—Å–æ–∫ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π
        """
        if not doc_ids:
            return []

        ready_docs = []
        start_time = time.time()

        print(f"\n‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {len(doc_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–º–∞–∫—Å {max_wait}—Å)...")

        checked_once = set()

        while time.time() - start_time < max_wait and len(ready_docs) < len(doc_ids):
            for doc_id in doc_ids:
                if doc_id in ready_docs:
                    continue

                status = self._check_rag_status(doc_id)

                if status in ['succeeded', 'indexed', 'completed']:
                    ready_docs.append(doc_id)
                    print(f"   ‚úÖ –ì–æ—Ç–æ–≤: {doc_id[:20]}...")
                elif status == 'failed':
                    if doc_id not in checked_once:
                        print(f"   ‚ùå –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {doc_id[:20]}...")
                        checked_once.add(doc_id)
                elif status == 'indexing':
                    if doc_id not in checked_once:
                        print(f"   ‚è≥ –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç—Å—è: {doc_id[:20]}...")
                        checked_once.add(doc_id)

            if len(ready_docs) < len(doc_ids):
                time.sleep(5)  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥

        print(f"üìä –ì–æ—Ç–æ–≤–æ –∫ –¥–æ–±–∞–≤–ª–µ–Ω–∏—é: {len(ready_docs)}/{len(doc_ids)}")
        return ready_docs

    def _check_rag_status(self, doc_id: str) -> str:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å RAG –∏–Ω–¥–µ–∫—Å–∞"""
        url = f"{self.base_url}/convai/knowledge-base/{doc_id}/rag-index"

        try:
            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code == 200:
                data = response.json()

                if 'indexes' in data and data['indexes']:
                    return data['indexes'][0].get('status', 'unknown')
                elif isinstance(data, dict) and 'status' in data:
                    return data.get('status', 'unknown')

            # –ï—Å–ª–∏ –Ω–µ—Ç –∏–Ω–¥–µ–∫—Å–∞, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –≥–æ—Ç–æ–≤ (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–Ω–¥–µ–∫—Å–∏—Ä—É—é—Ç—Å—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ)
            return 'succeeded'

        except Exception:
            return 'succeeded'  # –ü—Ä–∏ –æ—à–∏–±–∫–µ —Å—á–∏—Ç–∞–µ–º –≥–æ—Ç–æ–≤—ã–º

    # ===== –£–î–ê–õ–ï–ù–ò–ï –°–¢–ê–†–´–• –î–û–ö–£–ú–ï–ù–¢–û–í =====

    def safe_delete_old_documents(self, to_delete: List[Dict], new_docs_ready: bool) -> int:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        –£–¥–∞–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã
        """
        if not new_docs_ready and to_delete:
            print("‚ö†Ô∏è  –ù–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –≥–æ—Ç–æ–≤—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö")
            return 0

        if not to_delete:
            print("‚ÑπÔ∏è  –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
            return 0

        deleted_count = 0
        batch_size = 1000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–∏–π –∑–∞ 1 —Ü–∏–∫–ª
        pause_between_batches = 120  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)

        print(f"\nüóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ {len(to_delete)} —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤... (–ª–∏–º–∏—Ç –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ 1 –≤ 2 —Å–µ–∫—É–Ω–¥—ã, –º–∞–∫—Å {batch_size} –∑–∞ —Ä–∞–∑)")
        for batch_start in range(0, len(to_delete), batch_size):
            current_batch = to_delete[batch_start:batch_start+batch_size]
            print(f"\n=== –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_start//batch_size+1}: {len(current_batch)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ===")
            for j, doc_info in enumerate(current_batch, 1):
                doc_id = doc_info['id']
                doc_name = doc_info['name']
                reason = doc_info.get('reason', 'unknown')

                print(f"  {batch_start + j}/{len(to_delete)} - {doc_name[:40]}", end=" ")
                if self._delete_document(doc_id):
                    deleted_count += 1
                    self.sync_log['deletions'].append({
                        'id': doc_id,
                        'name': doc_name,
                        'deletion_date': datetime.now().isoformat(),
                        'reason': reason
                    })
                    self.save_sync_log()
                    print(f"‚úÖ")
                else:
                    print(f"‚ùå")
                time.sleep(2)  # –ë–æ–ª—å—à–µ –ø–∞—É–∑–∞ ‚Äî 2 —Å–µ–∫—É–Ω–¥—ã –º–µ–∂–¥—É —É–¥–∞–ª–µ–Ω–∏—è–º–∏
            if batch_start + batch_size < len(to_delete):
                print(f"  ‚è≥ –ü–∞—É–∑–∞ {pause_between_batches//60} –º–∏–Ω –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏...")
                time.sleep(pause_between_batches)
        return deleted_count

    def _delete_document(self, doc_id: str) -> bool:
        """–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID"""
        url = f"{self.base_url}/convai/knowledge-base/{doc_id}"

        try:
            response = requests.delete(url, headers=self.headers, timeout=30)
            return response.status_code in [200, 204]
        except Exception:
            return False

    # ===== –û–ë–ù–û–í–õ–ï–ù–ò–ï –ê–ì–ï–ù–¢–ê =====

    def update_agent_kb(self, ready_doc_ids: List[str]) -> bool:
        """
        –û–±–Ω–æ–≤–∏—Ç—å Knowledge Base –∞–≥–µ–Ω—Ç–∞

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É conversation_config
        """
        if not ready_doc_ids:
            log("‚ÑπÔ∏è  –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∫ –∞–≥–µ–Ω—Ç—É")
            return True

        agent_url = f"{self.base_url}/convai/agents/{self.agent_id}"

        log(f"\nü§ñ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
        log(f"   üìã –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {len(ready_doc_ids)}")

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        log(f"   üì• –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ (—Ç–∞–π–º–∞—É—Ç: 60—Å)...")
        try:
            response = requests.get(agent_url, headers=self.headers, timeout=60)
            log(f"   üì° HTTP {response.status_code}")

            if response.status_code != 200:
                log(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–≥–µ–Ω—Ç–∞: {response.status_code}")
                log(f"   üìù –û—Ç–≤–µ—Ç: {response.text[:300]}")
                return False

            agent_data = response.json()
            log(f"   ‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∞")
        except requests.exceptions.Timeout:
            log(f"   ‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞ (>60 —Å–µ–∫)")
            return False
        except Exception as e:
            log(f"   ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {type(e).__name__} - {str(e)[:200]}")
            return False

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        current_kb = agent_data.get('conversation_config', {}).get('knowledge_base', {})
        existing_ids = current_kb.get('ids', [])

        # –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞ ‚Äî –ü–ï–†–ï–ü–ò–°–´–í–ê–ï–ú knowledge_base.ids –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –Ω–∞–±–æ—Ä, –Ω–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ —Å—Ç–∞—Ä—ã–º–∏
        agent_kb_ids = ready_doc_ids[:50]  # –õ–∏–º–∏—Ç ElevenLabs ‚Äî –º–∞–∫—Å–∏–º—É–º 50

        update_data = {
            'conversation_config': {
                'knowledge_base': {
                    'type': 'knowledge_base',
                    'ids': agent_kb_ids
                }
            }
        }

        log(f"   üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {len(existing_ids)} ‚Üí {len(agent_kb_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ KB –∞–≥–µ–Ω—Ç–∞")
        log(f"   üìã –ü–µ—Ä–≤—ã–µ 5 ID –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {agent_kb_ids[:5] if agent_kb_ids else '–Ω–µ—Ç'}")
        log(f"   üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ PATCH –∑–∞–ø—Ä–æ—Å–∞ (—Ç–∞–π–º–∞—É—Ç: 300—Å)...")
        log(f"   üîó URL: {agent_url}")

        # –ü—ã—Ç–∞–µ–º—Å—è –æ–±–Ω–æ–≤–∏—Ç—å —Å retry
        wait_times = [15, 30, 60, 120, 180]
        for attempt in range(5):
            try:
                log(f"   üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/5...")
                start_time = time.time()
                log(f"   üïí Start PATCH-–∑–∞–ø—Ä–æ—Å–∞, –æ—Ç–ø—Ä–∞–≤–ª—è—é –¥–∞–Ω–Ω—ã–µ –∞–≥–µ–Ω—Ç—É...")
                connect_start = time.time()
                try:
                    response = requests.patch(
                        agent_url,
                        headers=self.headers,
                        json=update_data,
                        timeout=(15, 900)
                    )
                    connect_time = time.time() - connect_start
                    log(f"   ‚è±Ô∏è PATCH –∑–∞–≤–µ—Ä—à—ë–Ω –∑–∞ {connect_time:.1f}—Å, HTTP {response.status_code}")
                except Exception as e:
                    fail_time = time.time() - connect_start
                    log(f"   ‚ùå PATCH exception ({type(e).__name__}) –ø–æ—Å–ª–µ {fail_time:.1f}—Å: {str(e)}")
                    # log error to file
                    with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                        errlog.write(f"\n[{datetime.now().isoformat()}] PATCH exception (attempt {attempt+1}/5)\n")
                        errlog.write(f"URL: {agent_url}\n")
                        errlog.write(f"Payload IDs (total {len(agent_kb_ids)}): {agent_kb_ids[:5]}\n")
                        errlog.write(f"Exception: {type(e).__name__}: {str(e)}\n")
                        import traceback
                        errlog.write(traceback.format_exc())
                        errlog.write(f"Elapsed: {fail_time:.2f} sec\n")
                        errlog.write(f"Payload (truncated): {str(update_data)[:400]}\n")
                        errlog.write("-"*60+"\n")
                    raise

                elapsed = time.time() - start_time
                if response.status_code == 200:
                    log(f"   ‚úÖ –ê–≥–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω!")
                    return True
                else:
                    # log error to file
                    with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                        errlog.write(f"\n[{datetime.now().isoformat()}] PATCH HTTP error (attempt {attempt+1}/5)\n")
                        errlog.write(f"URL: {agent_url}\n")
                        errlog.write(f"Status: {response.status_code}, Text: {response.text[:400]}\n")
                        errlog.write(f"Payload IDs (total {len(agent_kb_ids)}): {agent_kb_ids[:5]}\n")
                        errlog.write(f"Elapsed: {elapsed:.2f} sec\n")
                        errlog.write(f"Payload (truncated): {str(update_data)[:400]}\n")
                        errlog.write("-"*60+"\n")
                    log(f"   ‚ùå HTTP {response.status_code}: {response.text[:300]}")
                    if attempt < 4:
                        wait_time = wait_times[attempt]
                        log(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–ø—ã—Ç–∫–æ–π {attempt+2}...")
                        time.sleep(wait_time)
                    else:
                        log(f"   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
                        return False

            except requests.exceptions.ConnectTimeout as e:
                elapsed = time.time() - start_time if 'start_time' in locals() else 0
                log(f"   ‚ùå –¢–∞–π–º–∞—É—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ API (>30 —Å–µ–∫): {str(e)}")
                with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                    errlog.write(f"\n[{datetime.now().isoformat()}] PATCH ConnectTimeout (attempt {attempt+1}/5)\n")
                    errlog.write(f"URL: {agent_url}\n")
                    errlog.write(f"Error: {str(e)}\n")
                    errlog.write(f"Elapsed: {elapsed:.2f} sec\n")
                    errlog.write("-"*60+"\n")
                if attempt < 2:
                    wait_time = (attempt + 1) * 15
                    log(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(wait_time)
                else:
                    log(f"   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã (—Ç–∞–π–º–∞—É—Ç—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è)")
                    return False
            except requests.exceptions.ReadTimeout as e:
                elapsed = time.time() - start_time if 'start_time' in locals() else 0
                log(f"   ‚ùå –¢–∞–π–º–∞—É—Ç —á—Ç–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–æ—à–ª–æ {elapsed:.1f}—Å –∏–∑ 300—Å): {str(e)}")
                with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                    errlog.write(f"\n[{datetime.now().isoformat()}] PATCH ReadTimeout (attempt {attempt+1}/5)\n")
                    errlog.write(f"URL: {agent_url}\n")
                    errlog.write(f"Error: {str(e)}\n")
                    errlog.write(f"Elapsed: {elapsed:.2f} sec\n")
                    errlog.write("-"*60+"\n")
                if attempt < 2:
                    wait_time = (attempt + 1) * 15
                    log(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(wait_time)
                else:
                    log(f"   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã (—Ç–∞–π–º–∞—É—Ç—ã —á—Ç–µ–Ω–∏—è)")
                    return False
            except requests.exceptions.Timeout as e:
                elapsed = time.time() - start_time if 'start_time' in locals() else 0
                log(f"   ‚ùå –û–±—â–∏–π —Ç–∞–π–º–∞—É—Ç PATCH –∑–∞–ø—Ä–æ—Å–∞ (–ø—Ä–æ—à–ª–æ {elapsed:.1f}—Å): {str(e)}")
                with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                    errlog.write(f"\n[{datetime.now().isoformat()}] PATCH Timeout (attempt {attempt+1}/5)\n")
                    errlog.write(f"URL: {agent_url}\n")
                    errlog.write(f"Error: {str(e)}\n")
                    errlog.write(f"Elapsed: {elapsed:.2f} sec\n")
                    errlog.write("-"*60+"\n")
                if attempt < 2:
                    wait_time = (attempt + 1) * 15
                    log(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(wait_time)
                else:
                    log(f"   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã (—Ç–∞–π–º–∞—É—Ç—ã)")
                    return False
            except requests.exceptions.RequestException as e:
                log(f"   ‚ùå –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {type(e).__name__} - {str(e)[:200]}")
                with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                    errlog.write(f"\n[{datetime.now().isoformat()}] PATCH RequestException (attempt {attempt+1}/5)\n")
                    errlog.write(f"URL: {agent_url}\n")
                    errlog.write(f"Error: {str(e)}\n")
                    errlog.write(f"Elapsed: {elapsed:.2f} sec\n")
                    errlog.write("-"*60+"\n")
                if attempt < 2:
                    wait_time = (attempt + 1) * 15
                    log(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(wait_time)
                else:
                    log(f"   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
                    return False
            except Exception as e:
                log(f"   ‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {type(e).__name__} - {str(e)[:200]}")
                import traceback
                with open("elevenlabs_patch_errors.log", "a", encoding="utf-8") as errlog:
                    errlog.write(f"\n[{datetime.now().isoformat()}] PATCH General Exception (attempt {attempt+1}/5)\n")
                    errlog.write(f"URL: {agent_url}\n")
                    errlog.write(f"Error: {type(e).__name__}: {str(e)}\n")
                    errlog.write(traceback.format_exc())
                    errlog.write(f"Elapsed: {elapsed:.2f} sec\n")
                    errlog.write(f"Payload (truncated): {str(update_data)[:400]}\n")
                    errlog.write("-"*60+"\n")
                if attempt < 2:
                    wait_time = (attempt + 1) * 15
                    log(f"   ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–æ–π...")
                    time.sleep(wait_time)
                else:
                    log(f"   ‚ùå –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
                    return False

        return False

    # ===== –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–ò =====

    def full_sync(self, files_dir: str = 'quarters', dry_run: bool = False, no_delete: bool = False, changed_files: List[str] = None):
        """
        –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π

        Args:
            files_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ñ–∞–π–ª–∞–º–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            dry_run: –ï—Å–ª–∏ True, —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
            no_delete: –ï—Å–ª–∏ True, –Ω–µ —É–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            changed_files: –°–ø–∏—Å–æ–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        log("=" * 70)
        log("üöÄ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–Ø ELEVENLABS KNOWLEDGE BASE")
        log("=" * 70)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞
        log(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        log(f"   - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Ñ–∞–π–ª–æ–≤: {files_dir}")
        log(f"   - –†–µ–∂–∏–º: {'DRY RUN' if dry_run else 'REAL'}")
        log(f"   - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö: {'–ù–ï–¢' if no_delete else '–î–ê'}")
        log(f"   - –ò–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(changed_files) if changed_files else '–≤—Å–µ'}")

        if dry_run:
            log("\n‚ö†Ô∏è  –†–ï–ñ–ò–ú DRY RUN (–±–µ–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n")

        # –®–∞–≥ 1: –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–Ω—É–∂–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π)
        log("\nüìö –®–∞–≥ 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ Knowledge Base...")
        
        # –í –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø—ã—Ç–∞–µ–º—Å—è –æ–±–Ω–æ–≤–∏—Ç—å –∫—ç—à (TTL: 1-2 —á–∞—Å–∞), –Ω–æ —Å fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
        if changed_files:
            log("   ‚ö° –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º: –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –µ—Å–ª–∏ —Å—Ç–∞—Ä—à–µ 1 —á–∞—Å–∞ (fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –∫—ç—à –¥–æ 24—á)")
            # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–µ–∂–∏–π –∫—ç—à (–¥–æ 1 —á–∞—Å–∞)
            all_docs = self.get_all_kb_documents_cached(ttl_minutes=60, use_cache_only=False)
            
            # –ï—Å–ª–∏ –∫—ç—à –±—ã–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª, –Ω–æ —Å—Ç–∞—Ä—ã–π –∫—ç—à –µ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ fallback
            if len(all_docs) == 0:
                log("   üì¶ –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∞—Ä—ã–π –∫—ç—à (–¥–æ 24 —á–∞—Å–æ–≤)...")
                all_docs = self.get_all_kb_documents_cached(ttl_minutes=60 * 24, use_cache_only=True)
        else:
            # –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è: –æ–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
            log("   üîÑ –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è: –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ API")
            all_docs = self.get_all_kb_documents_cached(ttl_minutes=60, use_cache_only=False)
        
        log(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ KB: {len(all_docs)}")
        
        if len(all_docs) == 0:
            if changed_files:
                log("   ‚ö†Ô∏è  –ö—ç—à –ø—É—Å—Ç–æ–π –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ")
                log("   üí° –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫—É –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ –Ω–µ —É–¥–∞–ª—è—é—Ç—Å—è)")
            else:
                log("   ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ KB")

        # –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —á—Ç–æ —É–¥–∞–ª–∏—Ç—å
        if changed_files:
            print(f"\nüéØ –ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï: {len(changed_files)} —Ñ–∞–π–ª–æ–≤")
            print(f"   –§–∞–π–ª—ã: {', '.join(changed_files)}\n")
            log("\nüîç –®–∞–≥ 2: –ü–æ–∏—Å–∫ —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        else:
            log("\nüîç –®–∞–≥ 2: –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è...")
        
        to_delete = self.identify_documents_to_delete(all_docs, changed_files=changed_files)
        log(f"   –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {len(to_delete)}")

        if to_delete:
            print("   –°–ø–∏—Å–æ–∫ –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ (–ø–µ—Ä–≤—ã–µ 10):")
            for doc in to_delete[:10]:
                print(f"      - {doc['name'][:50]} ({doc['reason']})")
            if len(to_delete) > 10:
                print(f"      ... –∏ –µ—â–µ {len(to_delete) - 10}")

        if dry_run:
            print("\n‚úÖ DRY RUN –∑–∞–≤–µ—Ä—à–µ–Ω (–∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã)")
            return

        # –®–∞–≥ 3: –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        print("\nüì§ –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        uploaded_ids = self.upload_new_documents(files_dir, changed_files=changed_files)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(uploaded_ids)}")

        # –®–∞–≥ 4: –ñ–¥–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
        ready_ids = []
        if uploaded_ids:
            log("\n‚è≥ –®–∞–≥ 4: –û–∂–∏–¥–∞–Ω–∏–µ RAG –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ñ–∞–π–ª–æ–≤
            # ~60 —Å–µ–∫—É–Ω–¥ –Ω–∞ —Ñ–∞–π–ª, –º–∏–Ω–∏–º—É–º 120, –º–∞–∫—Å–∏–º—É–º 10 –º–∏–Ω—É—Ç
            max_wait_seconds = max(120, min(60 * len(uploaded_ids), 600))
            log(f"   ‚è±Ô∏è  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è: {max_wait_seconds} —Å–µ–∫ ({len(uploaded_ids)} —Ñ–∞–π–ª–æ–≤)")
            ready_ids = self.wait_for_indexing(uploaded_ids, max_wait=max_wait_seconds)

            if len(ready_ids) == 0 and len(uploaded_ids) > 0:
                print("‚ö†Ô∏è  –ù–∏ –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞")
                new_docs_ready = False
            else:
                new_docs_ready = True
        else:
            new_docs_ready = True  # –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –º–æ–∂–Ω–æ —É–¥–∞–ª—è—Ç—å —Å—Ç–∞—Ä—ã–µ

        # –®–∞–≥ 5: –û–±–Ω–æ–≤–ª—è–µ–º –∞–≥–µ–Ω—Ç–∞
        if ready_ids:
            print("\nü§ñ –®–∞–≥ 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞...")
            agent_updated = self.update_agent_kb(ready_ids)
            new_docs_ready = agent_updated
        else:
            print("\nü§ñ –®–∞–≥ 5: –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è (–Ω–µ—Ç –Ω–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")

        # –®–∞–≥ 6: –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        deleted_count = 0
        if not no_delete:
            print("\nüóëÔ∏è  –®–∞–≥ 6: –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            deleted_count = self.safe_delete_old_documents(to_delete, new_docs_ready)
        else:
            print("\nüóëÔ∏è  –®–∞–≥ 6: –£–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ (--no-delete)")

        # –ò—Ç–æ–≥–∏
        print("\n" + "=" * 70)
        print("üìä –ò–¢–û–ì–ò –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–ò:")
        print(f"   üì§ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –Ω–æ–≤—ã—Ö: {len(uploaded_ids)}")
        print(f"   ‚úÖ –ü—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {len(ready_ids)}")
        print(f"   üóëÔ∏è  –£–¥–∞–ª–µ–Ω–æ —Å—Ç–∞—Ä—ã—Ö: {deleted_count}")
        print(f"   üìö –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ KB: ~{len(all_docs) + len(uploaded_ids) - deleted_count}")
        print("=" * 70)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–≥
        self.sync_log['last_sync'] = datetime.now().isoformat()
        self.save_sync_log()

        print("\n‚ú® –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
    import argparse

    parser = argparse.ArgumentParser(
        description='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è ElevenLabs Knowledge Base',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  %(prog)s                    # –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
  %(prog)s --dry-run          # –ü–æ–∫–∞–∑–∞—Ç—å —á—Ç–æ –±—É–¥–µ—Ç —Å–¥–µ–ª–∞–Ω–æ
  %(prog)s --no-delete        # –¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∞, –±–µ–∑ —É–¥–∞–ª–µ–Ω–∏—è
  %(prog)s --dir quarters     # –£–∫–∞–∑–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é —Å —Ñ–∞–π–ª–∞–º–∏
        """
    )
    parser.add_argument('--dir', default='quarters', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å MD —Ñ–∞–π–ª–∞–º–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: quarters)')
    parser.add_argument('--dry-run', action='store_true', help='–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è, –Ω–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å')
    parser.add_argument('--no-delete', action='store_true', help='–ù–µ —É–¥–∞–ª—è—Ç—å —Å—Ç–∞—Ä—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã')
    parser.add_argument('--changed-files', type=str, help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤')

    args = parser.parse_args()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω
    changed_files = None
    if args.changed_files:
        try:
            with open(args.changed_files, 'r', encoding='utf-8') as f:
                changed_files = [line.strip() for line in f if line.strip()]
        except FileNotFoundError:
            print(f"‚ö†Ô∏è  –§–∞–π–ª {args.changed_files} –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è")

    try:
        sync = ElevenLabsAutoSync()
        sync.full_sync(
            files_dir=args.dir,
            dry_run=args.dry_run,
            no_delete=args.no_delete,
            changed_files=changed_files
        )
        return 0
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
